botnet.py            34 ) # TODO: last step! (but actually the reward should be totally unused by non-learning botnets)
examples.py          32 ) # TODO: would be more practical if we had a class of list: [Fast, Fast_incr, QLearning...] (no need to give the botnet)
fast.py              54 ) # TODO: can be O(1) if computed during compute_min_time
fast_tentative.py    57 ) # TODO: Can be O(1) if computed during compute_min_time
learning_botnet.py   52 ) # TODO: makes not much sense to be able to choose any node
learning_botnet.py   53 ) # TODO: we could say that the Botnet starts with an already hijacked node instead (gives initial power)
learning_botnet.py   56 ) # TODO: implementation feasible in O(n) instead of O(n*ln(n)) (or maybe O(n²)...)
markov.py            8  ) # TODO: Implement online exploration with fixed depth (exact computation)
network.py           13 ) self.initial_power = initial_power  # TODO: change this by initial node?
network.py           45 ) # TODO: Un attribut accessibles (ensemble des noeuds accesibles depuis l'etat courant)
network.py           46 ) # TODO: It does not make any sense for the graph to be directed
network.py           107) # TODO: change to 1 - exp(-C*P/R) with C to adjust?
network.py           140) # TODO: Change cost?
network.py           141) # TODO: Make it depend on the success?
network.py           142) # TODO: Discrepancy between the final reward and this function when the state is full (except if action is None)
policy.py            31 ) # TODO: Change when it will change in network (--> network?)
qlearning.py         9  ) # TODO Understand initialization of the Q values
qlearning.py         10 ) # TODO Detect and eliminate blockades (decrease in reward during learning)
qlearning.py         11 ) # TODO Sparse sampling algorithm? / fixed-depth exploration variant?
qlearning.py         91 ) # TODO: find something more interesting?
qlearning.py         111) # TODO: Account for shaping if needed
qlearning.py         126) # # TODO: Back-propagation
qlearning.py         149) # TODO: Back-propagates the max_a Q(s, a) on each state to speed-up learning
reward_incr.py       31 ) # TODO: Find a way to make it stable, i.e. if the order of the nodes is optimal then so will be the result
strategy.py          4  ) # TODO: Delete this file (the smart code is in the exploration, not in the strategy)
tests.py             67 ) if success and botnet.state.add(action).is_full():  # TODO: include all this in network somehow
thompson_sampling.py 7  ) # TODO Ajouter de l'auto-évaluation des stratégies adoptées, s'en servir pour les retenir, et détecter des blocages.
thompson_sampling.py 8  ) # TODO: Initialization of successes/failures
thompson_sampling.py 49 ) # success, trials = 0, 0 TODO
thompson_sampling.py 70 ) return 1  # TODO
thompson_sampling.py 81 ) return trials - 1   # Because of initialization, TODO
thompson_sampling.py 93 ) def be_curious(self):  # TODO: merge with exploration?
thompson_sampling.py 122) # return self.policy(state) TODO
thompson_sampling.py 137) max_line = self.get_best_actions(self.state.add(action))[0]  # TODO: sure of the state.add()?
thompson_sampling.py 185) # TODO: add doc
thompson_sampling.py 202) self.memory = []   # TODO: add doc
thompson_sampling.py 203) self.history = []  # TODO: add doc
thompson_sampling.py 219) # TODO: change this
thompson_sampling.py 230) #     expected_value = self.network.total_power / (1 - self.gamma)  # Some constant ? TODO
thompson_sampling.py 278) # TODO: add doc
thompson_sampling.py 310) # success, trials = 0, 0 TODO
thompson_sampling.py 329) return 1  # TODO
